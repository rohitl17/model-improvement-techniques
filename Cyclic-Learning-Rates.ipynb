{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications.densenet import DenseNet121\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler,ReduceLROnPlateau,CSVLogger,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score,roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score,classification_report,auc,roc_curve\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def img_standardization(x):\n",
    "    \n",
    "#     x=cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x=x.astype('float16')/255.\n",
    "#     return x\n",
    "    return ((x-imagenet_mean)/imagenet_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom callback function to calculate average ROC,F1 and Kappa score\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):\n",
    "        self.X_val = validation_data[0]\n",
    "        self.Y_val = validation_data[1]\n",
    "        \n",
    "#         X_val=np.zeros(self.x_val.shape,dtype='float32')\n",
    "#         for index in range(self.x_val.shape[0]):\n",
    "#             X_val[index]=img_standardization(self.x_val[index])\n",
    "#         self.X_val=X_val\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if(epoch%1==0):\n",
    "            print(\"Calc Roc\")\n",
    "            all_rocs = []\n",
    "            y_pred_val = self.model.predict(self.X_val,verbose=1)\n",
    "\n",
    "            try:\n",
    "                roc_val = roc_auc_score(self.Y_val, y_pred_val)\n",
    "                all_rocs.append(roc_val)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            all_rocs = np.array(all_rocs)\n",
    "            mean_roc = np.mean(all_rocs)\n",
    "            \n",
    "            Y_val_kappa=[np.argmax(i) for i in self.Y_val]\n",
    "            y_pred_kappa=[np.argmax(i) for i in y_pred_val]\n",
    "                        \n",
    "            avg_f1 = f1_score(Y_val_kappa, y_pred_kappa, average='weighted')\n",
    "            kappa_score = cohen_kappa_score(Y_val_kappa, y_pred_kappa) \n",
    "            \n",
    "            print(\"Mean ROC VAL {0}\".format(mean_roc))\n",
    "            print(\"AVG F1 {0}\".format(avg_f1))\n",
    "            print(\"KAPPA {0}\".format(kappa_score))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from keras.callbacks import LambdaCallback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LRFinder:\n",
    "    \"\"\"\n",
    "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
    "    See for details:\n",
    "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=32, epochs=1):\n",
    "        # If x_train contains data for multiple inputs, use length of the first input.\n",
    "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
    "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
    "\n",
    "        # Compute number of batches and LR multiplier\n",
    "        num_batches = epochs * N / batch_size\n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit(x_train, y_train,\n",
    "                       batch_size=batch_size, epochs=epochs,\n",
    "                       callbacks=[callback], verbose=1)\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
    "        if steps_per_epoch is None:\n",
    "            try:\n",
    "                steps_per_epoch = len(generator)\n",
    "            except (ValueError, NotImplementedError) as e:\n",
    "                raise e('`steps_per_epoch=None` is only valid for a'\n",
    "                        ' generator based on the '\n",
    "                        '`keras.utils.Sequence`'\n",
    "                        ' class. Please specify `steps_per_epoch` '\n",
    "                        'or use the `keras.utils.Sequence` class.')\n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
    "                                                      logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit_generator(generator=generator,\n",
    "                                 epochs=epochs,\n",
    "                                 steps_per_epoch=steps_per_epoch,\n",
    "                                 callbacks=[callback], verbose=1,\n",
    "                                 **kw_fit)\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale(x_scale)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
    "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(lrs, derivatives)\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)\n",
    "        plt.show()\n",
    "\n",
    "    def get_derivatives(self, sma):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
    "        return derivatives\n",
    "\n",
    "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
    "        derivatives = self.get_derivatives(sma)\n",
    "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])[0]\n",
    "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size = 32):\n",
    "    indices = np.arange(len(X))\n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices)\n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    x=img_standardization(X[batch])\n",
    "                    yield x, Y[batch]\n",
    "                    batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val=np.zeros(x_val.shape,dtype='float32')\n",
    "for index in range(x_val.shape[0]):\n",
    "    X_val[index]=img_standardization(x_val[index])\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=[]\n",
    "for i in y_train:\n",
    "    if i.any()!=1:\n",
    "        Y_train.append(0)\n",
    "    else:\n",
    "        Y_train.append(1)\n",
    "Y_train=to_categorical(Y_train,num_classes=2)\n",
    "\n",
    "Y_val=[]\n",
    "for i in y_val:\n",
    "    if i.any()!=1:\n",
    "        Y_val.append(0)\n",
    "    else:\n",
    "        Y_val.append(1)\n",
    "Y_val=to_categorical(Y_val,num_classes=2)\n",
    "\n",
    "roc_call = roc_callback((X_val,Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conversion of labels into One-hot encoding\n",
    "Y_train[np.where(Y_train==2)]=1\n",
    "Y_val[np.where(Y_val==2)]=1\n",
    "\n",
    "Y_train=to_categorical(Y_train,num_classes=2)\n",
    "Y_val=to_categorical(Y_val,num_classes=2)\n",
    "\n",
    "roc_call = roc_callback((X_val,Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_val.shape)\n",
    "print (Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam=Adam(lr=1e-4)\n",
    "\n",
    "#Model declaration:Densenet \n",
    "dense_model = DenseNet121(weights='imagenet', include_top=False,input_shape=(224,224,3),pooling='avg')\n",
    "preds = Dense(14,activation='sigmoid')(dense_model.output)\n",
    "model = Model(dense_model.input,preds)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.load_weights('/opt/bucketdata/Users/Rohit/15_pathology_model/NIH_more_than_decent_weights/updated_best_weights.h5')\n",
    "\n",
    "\n",
    "#Model parameters for compiling\n",
    "\n",
    "model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = Dense(2,activation='softmax')(model.get_layer('avg_pool').output)\n",
    "new_model = Model(model.input,new_preds)\n",
    "new_model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_true=np.argmax(Y_train,axis=-1)\n",
    "weights = class_weight.compute_class_weight('balanced',np.unique(y_true),y_true)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_model.load_weights(\"./Weights/CHAI_PvsNP_newlabels_transferlearned_fromNIHPvsNP.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.find_generator(batch_generator(X_train, Y_train), start_lr=1e-6, end_lr=1e-3, steps_per_epoch=len(X_train)/32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.plot_loss(n_skip_beginning=5, n_skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.plot_loss_change(sma=20, n_skip_beginning=5, n_skip_end=5, y_lim=(-0.01, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"Healthy\", \"Pathology\"]\n",
    "\n",
    "# define the minimum learning rate, maximum learning rate, batch size,\n",
    "# step size, CLR method, and number of epochs\n",
    "MIN_LR = 1e-6\n",
    "MAX_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "STEP_SIZE = 4\n",
    "CLR_METHOD = \"exp_range\" #triangular2, triangular\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# define the path to the output training history plot and cyclical\n",
    "# learning rate plot\n",
    "#TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_plot.png\"])\n",
    "#CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot.png\"])\n",
    "\n",
    "\n",
    "clr = CyclicLR(\n",
    "\tmode=CLR_METHOD,\n",
    "\tbase_lr=MIN_LR,\n",
    "\tmax_lr=MAX_LR,\n",
    "\tstep_size= STEP_SIZE * (X_train.shape[0] // BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback for saving model weights based on minimum Validation loss \n",
    "model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join('./Weights/', 'NIH_PvsNP_19022020_clr.hdf5'),\n",
    "        monitor='val_loss', mode='min',save_best_only=True, verbose=1, save_weights_only=True)\n",
    "\n",
    "#Callback for reducing learning rate based on validation loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=5, min_delta=0.0001, verbose=1, min_lr=0.000001)\n",
    "\n",
    "#Callback for storing logs of learning rate, loss and accuracy\n",
    "csvlogger=CSVLogger('NIH_PvsNP_19022020.csv')\n",
    "\n",
    "callbacks = [model_checkpoint, roc_call,csvlogger, clr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit_generator(batch_generator_train(X_1, Y_train, batch_size=32),validation_data=batch_generator_val(X_val, Y_val, batch_size=32),steps_per_epoch=len(X_1) / 32, epochs=10,callbacks=callbacks,verbose=1,validation_steps=len(X_val) / 32)\n",
    "new_model.fit_generator(batch_generator(X_train, Y_train, batch_size=32),validation_data=(X_val, Y_val),steps_per_epoch=len(X_train) / 32, epochs=100,callbacks=callbacks,verbose=1, shuffle=True, class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_model.load_weights(\"/opt/bucketdata/Users/gunjan/CHAI/tb_data_experiments/other_analysis/model_creation/hp/data_generator/py_files/combined_data/(nih+CHAI)>CHAI/CHAI_hp_adam1e-6_denseblock_freezed.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(\"./Weights/NIH_PvsNP_19022020_clr.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=[]\n",
    "# for index in range(X_val.shape[0]):\n",
    "#     X_test.append(img_standardization(X_val[index]))\n",
    "# X_test=np.array(X_test)\n",
    "\n",
    "op=new_model.predict(X_val,verbose=1)\n",
    "# op=model.predict_generator(batch_generator_val(x_val, Y_val, batch_size=1),steps=len(X_val) /1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=[]\n",
    "for i in op:\n",
    "    if i[1]>=0.5:\n",
    "        Y_pred.append(1)\n",
    "    else:\n",
    "        Y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=np.argmax(op,axis=1)\n",
    "Y_test=np.argmax(Y_val,axis=-1)\n",
    "# Y_test=Y_val.copy()\n",
    "target_names = ['normal','Pathology']\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print (confusion_matrix(Y_test,Y_pred))\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test,Y_pred,target_names=target_names))\n",
    "\n",
    "print(\"Kappa score:\")\n",
    "print(cohen_kappa_score(Y_test,Y_pred))\n",
    "\n",
    "# Converting Healthy+other into NonTB cases to calculate ROC\n",
    "y_true=np.array(Y_test)\n",
    "y_true[np.where(y_true==1)]=1\n",
    "y_true[np.where(y_true==2)]=1\n",
    "\n",
    "#Taking probabilities of TB class\n",
    "preds = op[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# ROC curve plot\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig('TB_ROC_curve.png',dpi=200)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
